{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cYPZMYrjlay"
      },
      "source": [
        "## **Import libs:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_Z0mreftNrMc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.tv_tensors import Image, Mask\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "import torchvision.transforms.v2 as T\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "HSoqh2TQJ2zZ"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "BATCHSIZE = 2 # Further reduced batch size to mitigate OutOfMemoryError\n",
        "LR = 1e-5 # Further reduced learning rate for stability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_DVdhVjqTe"
      },
      "source": [
        "## **Check Gpu:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNNT5iJuJxDg",
        "outputId": "72e00830-e2d5-45eb-b3c0-7bfc2afa0898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9YeMxafjxEt"
      },
      "source": [
        "## **Architecture:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8nhTpSt69GQ"
      },
      "source": [
        "Pax merging is the equivalent to MaxPool 2x2 is the Unet.\n",
        "I divide the input as an image composed by 4x4 patches.\n",
        "I want to create a image composed by 2x2 patches, so I need to reshape.\n",
        "\n",
        "x.reshape(B, H // 2, 2, W // 2, 2, C) give us (B, H // 2, 2, W // 2, 2, C), but I want B, H//2, W//2, 2C... I need to permute, so I have: B, H//2, W//2, 2, 2, C.\n",
        "After that, I need to reshape to have B, H//2, W//2, 4C, and I need a reducton to have B, H//2, W//2, 2C\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "E_ElVy0z69GQ"
      },
      "outputs": [],
      "source": [
        "class PatchMerging(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim * 4)\n",
        "        self.reduction = nn.Linear(dim * 4, 2*dim, bias = False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, H, W, C = x.shape\n",
        "        x = x.reshape(B, H // 2, 2, W // 2, 2, C).permute(0, 1, 3, 2, 4, 5).reshape(B, H // 2, W // 2, 4 * C)\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_kDzlju69GR"
      },
      "source": [
        "We also need the equivalent of Upsampling: Patch Expanding..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N1SFGvx-69GR"
      },
      "outputs": [],
      "source": [
        "class PatchExpanding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.expand = nn.Linear(2 * dim, 4 * dim, bias = False)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = B, H /2, W/2, 2C\n",
        "        x = self.expand(x)\n",
        "        # x = B, H/2, W/2, 4C\n",
        "        B, semiH, semiW, Cx4 = x.shape\n",
        "        x = x.view(B, semiH, semiW, 2, 2, Cx4 // 4)\n",
        "        # x = B, H/2, W/2, 2, 2, 4C/4\n",
        "        x = x.permute(0, 1, 3, 2, 4, 5)\n",
        "        # x = B, H/2, 2, W/2, 2, 4C/4\n",
        "        x = x.reshape(B, semiH *2 , semiW * 2,Cx4 // 4)\n",
        "        # x = B, H, W, 4C/4\n",
        "        x = self.norm(x)\n",
        "        return x # B, H, W, C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhlX4ZTs69GR"
      },
      "source": [
        "One of the assets of swin unet, it's this capacity of create tokens from the input, using windows, and doing attention on fixed windows W-MSA vs on swift windows SW-MSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TmC939uH69GR"
      },
      "outputs": [],
      "source": [
        "def window_partition(x, window_size):\n",
        "    B, H, W, C = x.shape\n",
        "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
        "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
        "    return windows\n",
        "\n",
        "def window_reverse(windows, window_size, H, W):\n",
        "    C = windows.shape[-1]\n",
        "    x = windows.view(-1, H //window_size, W // window_size, window_size, window_size, C)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, H, W, C)\n",
        "    return x\n",
        "\n",
        "def get_relative_position_index(window_size, device = None):\n",
        "    coords_h = torch.arange(window_size, device= device, dtype=torch.long)\n",
        "    coords_w = torch.arange(window_size, device= device, dtype=torch.long)\n",
        "    coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))  # 2, Wh, Ww\n",
        "    coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "    relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "    relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "    relative_coords[:, :, 0] += window_size - 1  # shift to start from 0\n",
        "    relative_coords[:, :, 1] += window_size - 1\n",
        "    relative_coords[:, :, 0] *= 2 * window_size - 1\n",
        "    relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "    return relative_position_index\n",
        "\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        self.register_buffer(\"relative_position_index\",  get_relative_position_index(window_size))\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads)\n",
        "        )\n",
        "\n",
        "        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B_, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))\n",
        "\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            N, N, -1)\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, N, N)\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "        else:\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5vn6P4r69GS"
      },
      "source": [
        "Swin Transformer block:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qp9sZSUm69GS"
      },
      "outputs": [],
      "source": [
        "class SwinTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = WindowAttention(dim, window_size, num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        self.drop_path = nn.Identity() if drop_path == 0. else nn.Dropout(drop_path)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.Linear(mlp_hidden_dim, dim),\n",
        "            nn.Dropout(drop)\n",
        "        )\n",
        "\n",
        "        if shift_size > 0:\n",
        "            self.register_buffer(\"attention_mask\", self.compute_mask(*input_resolution, device = \"cpu\"))\n",
        "        else:\n",
        "            self.attention_mask = None\n",
        "\n",
        "    def compute_mask(self, H, W, device = None):\n",
        "        img_mask = torch.zeros((1, H, W, 1), device=device)\n",
        "        h_slices = (slice(0, -self.window_size),\n",
        "                    slice(-self.window_size, -self.shift_size),\n",
        "                    slice(-self.shift_size, None))\n",
        "        w_slices = (slice(0, -self.window_size),\n",
        "                    slice(-self.window_size, -self.shift_size),\n",
        "                    slice(-self.shift_size, None))\n",
        "        cnt = 0\n",
        "        for h in h_slices:\n",
        "            for w in w_slices:\n",
        "                img_mask[:, h, w, :] = cnt\n",
        "                cnt += 1\n",
        "\n",
        "        mask_windows = window_partition(img_mask, self.window_size)\n",
        "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
        "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
        "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(\"-inf\")).masked_fill(attn_mask == 0, float(0.0))\n",
        "        return attn_mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.attention_mask is not None:\n",
        "            self.attention_mask = self.attention_mask.to(x.device)\n",
        "        B, H, W, C = x.shape\n",
        "        shortcut = x\n",
        "\n",
        "        x = self.norm1(x)\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        x_windows = window_partition(x, self.window_size)\n",
        "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n",
        "\n",
        "        attn_windows = self.attn(x_windows, mask=self.attention_mask)\n",
        "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
        "\n",
        "        x = window_reverse(attn_windows, self.window_size, H, W)\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "\n",
        "        x = shortcut + x\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gKBJ4WP69GS"
      },
      "source": [
        "Swin Transformer (W-MWSA & SW-MSA):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "iMzxRRiR69GS"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dim, input_resolution, depth, num_heads, window_size=7):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [SwinTransformerBlock(dim=dim,\n",
        "                                  input_resolution=input_resolution,\n",
        "                                  num_heads=num_heads,\n",
        "                                  window_size=window_size,\n",
        "                                  shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
        "                                  mlp_ratio=4.,\n",
        "                                  qkv_bias=True,\n",
        "                                  drop=0.,\n",
        "                                  attn_drop=0.,\n",
        "                                  drop_path=0.) for i in range(depth)])\n",
        "        self.downsample = PatchMerging(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        skip_connections = x\n",
        "        x = self.downsample(x)\n",
        "        return x, skip_connections\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dim, input_resolution, depth, num_heads, window_size=7):\n",
        "        super().__init__()\n",
        "        self.upsample = PatchExpanding(dim)\n",
        "        self.fusin = nn.Linear(2 * dim, dim)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [SwinTransformerBlock(dim=dim,\n",
        "                                  input_resolution=input_resolution,\n",
        "                                  num_heads=num_heads,\n",
        "                                  window_size=window_size,\n",
        "                                  shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
        "                                  mlp_ratio=4.,\n",
        "                                  qkv_bias=True,\n",
        "                                  drop=0.,\n",
        "                                  attn_drop=0.,\n",
        "                                  drop_path=0.) for i in range(depth)])\n",
        "    def forward(self, x, skip_connections):\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat((x, skip_connections), -1)\n",
        "        x = self.fusin(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX_I9NdZ69GS",
        "outputId": "27f13061-3933-404d-d1ab-8774b03e6b7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SwinUnet(\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (proj): Conv2d(3, 96, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (encoder): ModuleList(\n",
            "    (0): Encoder(\n",
            "      (blocks): ModuleList(\n",
            "        (0-1): 2 x SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "          (drop_path): Identity()\n",
            "          (mlp): Sequential(\n",
            "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (downsample): PatchMerging(\n",
            "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (1): Encoder(\n",
            "      (blocks): ModuleList(\n",
            "        (0-1): 2 x SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "          (drop_path): Identity()\n",
            "          (mlp): Sequential(\n",
            "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=768, out_features=192, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (downsample): PatchMerging(\n",
            "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (2): Encoder(\n",
            "      (blocks): ModuleList(\n",
            "        (0-1): 2 x SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (drop_path): Identity()\n",
            "          (mlp): Sequential(\n",
            "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (downsample): PatchMerging(\n",
            "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (3): Encoder(\n",
            "      (blocks): ModuleList(\n",
            "        (0-1): 2 x SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (drop_path): Identity()\n",
            "          (mlp): Sequential(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (downsample): PatchMerging(\n",
            "        (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
            "        (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): ModuleList(\n",
            "    (0): Decoder(\n",
            "      (upsample): PatchExpanding(\n",
            "        (expand): Linear(in_features=1536, out_features=3072, bias=False)\n",
            "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (fusin): Linear(in_features=1536, out_features=768, bias=True)\n",
            "      (blocks): ModuleList(\n",
            "        (0-1): 2 x SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (drop_path): Identity()\n",
            "          (mlp): Sequential(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): Decoder(\n",
            "      (upsample): PatchExpanding(\n",
            "        (expand): Linear(in_features=768, out_features=1536, bias=False)\n",
            "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (fusin): Linear(in_features=768, out_features=384, bias=True)\n",
            "      (blocks): ModuleList(\n",
            "        (0-1): 2 x SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (drop_path): Identity()\n",
            "          (mlp): Sequential(\n",
            "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): Decoder(\n",
            "      (upsample): PatchExpanding(\n",
            "        (expand): Linear(in_features=384, out_features=768, bias=False)\n",
            "        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (fusin): Linear(in_features=384, out_features=192, bias=True)\n",
            "      (blocks): ModuleList(\n",
            "        (0-1): 2 x SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "          (drop_path): Identity()\n",
            "          (mlp): Sequential(\n",
            "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=768, out_features=192, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (3): Decoder(\n",
            "      (upsample): PatchExpanding(\n",
            "        (expand): Linear(in_features=192, out_features=384, bias=False)\n",
            "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (fusin): Linear(in_features=192, out_features=96, bias=True)\n",
            "      (blocks): ModuleList(\n",
            "        (0-1): 2 x SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "          (drop_path): Identity()\n",
            "          (mlp): Sequential(\n",
            "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (bottleneck): Sequential(\n",
            "    (0): SwinTransformerBlock(\n",
            "      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): WindowAttention(\n",
            "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      (drop_path): Identity()\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=6144, out_features=1536, bias=True)\n",
            "        (4): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (1): SwinTransformerBlock(\n",
            "      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): WindowAttention(\n",
            "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      (drop_path): Identity()\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=6144, out_features=1536, bias=True)\n",
            "        (4): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (head): Conv2d(96, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (final_upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=512, patch_size=2, in_chans=3, embed_dim=96):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x)\n",
        "        x = x.permute(0, 2, 3, 1).contiguous()\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SwinUnet(nn.Module):\n",
        "    def __init__(self, img_size = 256, in_chans=3, num_classes=3, embed_dim=96, depths=[2,2,2,2], patch_size=2, num_heads=[3,6,12,24], window_size=8): # Changed window_size to 8 and num_classes to 3\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size = patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        self.encoder = nn.ModuleList()\n",
        "        self.decoder = nn.ModuleList()\n",
        "        self.dim = embed_dim\n",
        "\n",
        "        dim = embed_dim\n",
        "        resolution = img_size // patch_size\n",
        "        for i in range(len(depths)):\n",
        "            self.encoder.append(Encoder(dim=dim, input_resolution=(resolution, resolution), depth=depths[i], num_heads=num_heads[i], window_size=window_size))\n",
        "            dim *= 2\n",
        "            resolution = resolution // 2\n",
        "\n",
        "        # Fix: The bottleneck should use the full 'dim' (1536) from the last encoder, not dim // 2\n",
        "        self.bottleneck = nn.Sequential(*[SwinTransformerBlock(dim=dim, # Changed from dim//2 to dim\n",
        "                                                               input_resolution=(resolution, resolution),\n",
        "                                                               num_heads=num_heads[-1],\n",
        "                                                               window_size=window_size,\n",
        "                                                               shift_size= window_size // 2,\n",
        "                                                               mlp_ratio=4.,\n",
        "                                                               qkv_bias=True,\n",
        "                                                               drop=0.,\n",
        "                                                               attn_drop=0.,\n",
        "                                                               drop_path=0.) for i in range(depths[-1])])\n",
        "        self.decoder = nn.ModuleList()\n",
        "        for i in reversed(range(len(depths))):\n",
        "            dim = dim // 2\n",
        "            resolution = resolution * 2\n",
        "            self.decoder.append(Decoder(dim=dim, input_resolution=(resolution, resolution), depth=depths[i], num_heads=num_heads[i], window_size=window_size))\n",
        "\n",
        "        self.head = nn.Conv2d(embed_dim, num_classes, kernel_size=1)\n",
        "        self.final_upsample = nn.Upsample(\n",
        "            scale_factor=patch_size,\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        skip_connections = []\n",
        "        for enc in self.encoder:\n",
        "            x, skip = enc(x)\n",
        "            skip_connections.append(skip)\n",
        "        x = self.bottleneck(x)\n",
        "        for i, dec in enumerate(self.decoder):\n",
        "            skip = skip_connections[-(i+1)]\n",
        "            x = dec(x, skip)\n",
        "        x = x.permute(0, 3, 1, 2).contiguous()\n",
        "        x = self.head(x)\n",
        "        x = self.final_upsample(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "model = SwinUnet().to(device)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wc2sgIXj0Lt"
      },
      "source": [
        "##**DataGenerator:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "uram423_j4HU"
      },
      "outputs": [],
      "source": [
        "train_transform = T.Compose([\n",
        "    T.Resize((256, 256)),\n",
        "    T.RandomHorizontalFlip(p = 0.5),\n",
        "    T.RandomRotation(degrees = 15),\n",
        "])\n",
        "\n",
        "val_transform = T.Compose([\n",
        "    T.Resize((256, 256)),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Zpdy8rpJkhup"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "import torchvision.transforms.v2 as T\n",
        "\n",
        "\n",
        "\n",
        "class OxfordPetSegmentation(Dataset):\n",
        "    def __init__(self, root, split = \"trainval\", transforms = None):\n",
        "        self.dataset = OxfordIIITPet(root=root, download=True, split = split, target_types = [\"segmentation\", \"category\"])\n",
        "        self.transforms = transforms\n",
        "        self.classes = self.dataset.classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, (mask, label) = self.dataset[idx]\n",
        "\n",
        "        mask_np = np.array(mask)\n",
        "        animal_pixels = (mask_np == 1)\n",
        "\n",
        "        multiclass_mask = np.zeros_like(mask_np, dtype=np.uint8)\n",
        "\n",
        "        breed_name = self.classes[label].lower()\n",
        "        if breed_name in ['abyssinian', 'bengal', 'birman', 'bombay', 'british shorthair',\n",
        "                        'egyptian mau', 'maine coon', 'persian', 'ragdoll', 'russian blue',\n",
        "                        'siamese', 'sphynx']: # Cat\n",
        "            multiclass_mask[animal_pixels] = 1\n",
        "        else:\n",
        "            multiclass_mask[animal_pixels] = 2\n",
        "\n",
        "        image_dp = Image(image)\n",
        "        mask_dp = Mask(multiclass_mask)\n",
        "\n",
        "        # Apply geometric transforms to datapoints\n",
        "        if self.transforms:\n",
        "            # Transforms that work on (dp.Image, dp.Mask) will keep them synchronized\n",
        "            image_dp, mask_dp = self.transforms(image_dp, mask_dp)\n",
        "\n",
        "        image_tensor = T.ToDtype(torch.float32, scale=True)(image_dp)\n",
        "        mask_tensor = torch.as_tensor(mask_dp, dtype=torch.long)\n",
        "\n",
        "        return image_tensor, mask_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "IWLutCzzDqkp"
      },
      "outputs": [],
      "source": [
        "train_datset = OxfordPetSegmentation(root = \"data\", split = \"trainval\", transforms = train_transform)\n",
        "val_datset = OxfordPetSegmentation(root = \"data\", split = \"test\", transforms = val_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA8i7elGMjJg",
        "outputId": "77670d03-a0f5-490e-e0ae-9c7a398c6d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 256, 256])\n",
            "torch.Size([256, 256])\n",
            "tensor([0, 1])\n"
          ]
        }
      ],
      "source": [
        "img, mask = train_datset[0]\n",
        "print(img.shape)    # [3, 512, 512]\n",
        "print(mask.shape)   # [512, 512]\n",
        "print(torch.unique(mask))  # tensor([0, 1, 2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcyh-zWDGS_3",
        "outputId": "06690f42-6dbc-4b18-e559-c5a0e136eae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_datset, batch_size = BATCHSIZE, shuffle = True, num_workers= 4)\n",
        "val_loader = DataLoader(val_datset, batch_size = BATCHSIZE, shuffle = True, num_workers= 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTmK29a5I6G-"
      },
      "source": [
        "##**Training:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "G23qjYl5I8bu"
      },
      "outputs": [],
      "source": [
        "# Loss: CrossEntropyLoss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ks5ToMezJJEo"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=\"Train\", leave = False)\n",
        "    for images, masks in pbar:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix({\"loss\": loss.item()})\n",
        "    return running_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SxmjG6yjGbod"
      },
      "outputs": [],
      "source": [
        "def compute_confusion(preds, targets, class_id):\n",
        "    \"\"\"\n",
        "    preds, targets: [B, H, W] torch tensors\n",
        "    class_id: int (1=cat, 2=dog)\n",
        "    \"\"\"\n",
        "    preds_c = preds == class_id\n",
        "    targets_c = targets == class_id\n",
        "\n",
        "    tp = (preds_c & targets_c).sum().item()\n",
        "    fp = (preds_c & ~targets_c).sum().item()\n",
        "    fn = (~preds_c & targets_c).sum().item()\n",
        "\n",
        "    return tp, fp, fn\n",
        "\n",
        "\n",
        "\n",
        "def precision_recall_f1_iou(tp, fp, fn, eps=1e-8):\n",
        "    precision = tp / (tp + fp + eps)\n",
        "    recall    = tp / (tp + fn + eps)\n",
        "    f1        = 2 * precision * recall / (precision + recall + eps)\n",
        "    iou       = tp / (tp + fp + fn + eps)\n",
        "    return precision, recall, f1, iou\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "efYGnA2ZJjHe"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validate_one_epoch(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    stats = {\n",
        "        \"cat\" : {\"tp\": 0, \"fp\": 0, \"fn\": 0},\n",
        "        \"dog\" : {\"tp\": 0, \"fp\": 0, \"fn\": 0}\n",
        "    }\n",
        "\n",
        "\n",
        "    for images, masks in dataloader:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        for class_idx, key in enumerate([\"cat\", \"dog\"]):\n",
        "            tp, fp, fn = compute_confusion(preds, masks, class_idx + 1) # Because class are 1 and 2\n",
        "            stats[key][\"tp\"] += tp\n",
        "            stats[key][\"fp\"] += fp\n",
        "            stats[key][\"fn\"] += fn\n",
        "    metrics = {}\n",
        "    for key in [\"cat\", \"dog\"]:\n",
        "        tp = stats[key][\"tp\"]\n",
        "        fp = stats[key][\"fp\"]\n",
        "        fn = stats[key][\"fn\"]\n",
        "\n",
        "        precision, recall, f1, iou = precision_recall_f1_iou(tp, fp, fn, eps = 1e-8)\n",
        "        metrics[key] = {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"iou\": iou,\n",
        "        }\n",
        "    return running_loss / len(dataloader), metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "qdOzI_XYJxu4",
        "outputId": "64cea810-23e3-45e0-ba63-c3e16b7713da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1954637135.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     mean_iou = (\n",
            "\u001b[0;32m/tmp/ipython-input-715050043.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m     def backward(\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     ):\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "best_iou = 0.0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_metrics = validate_one_epoch(model, val_loader, criterion, device)\n",
        "    mean_iou = (\n",
        "        val_metrics[\"cat\"][\"iou\"] +\n",
        "        val_metrics[\"dog\"][\"iou\"]\n",
        "    ) / 2\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"Val   Loss: {val_loss:.4f}\")\n",
        "    print(\n",
        "        f\"Mean IoU: {mean_iou:.3f} | \"\n",
        "        f\"Cat IoU: {val_metrics['cat']['iou']:.3f} | \"\n",
        "        f\"Dog IoU: {val_metrics['dog']['iou']:.3f}\"\n",
        "    )\n",
        "\n",
        "    for cls in [\"cat\", \"dog\"]:\n",
        "        m = val_metrics[cls]\n",
        "        print(\n",
        "            f\"{cls.upper()}  \"\n",
        "            f\"P: {m['precision']:.3f} \"\n",
        "            f\"R: {m['recall']:.3f} \"\n",
        "            f\"F1: {m['f1']:.3f}\"\n",
        "        )\n",
        "\n",
        "    if mean_iou > best_iou:\n",
        "        best_iou = mean_iou\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(\"Best model saved\")\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), \"final_model.pth\")\n",
        "\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XwooMw2KHEh"
      },
      "source": [
        "##**Testing:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roJTeDvNKJ6E"
      },
      "outputs": [],
      "source": [
        "def compute_metrics_binary(pred, target):\n",
        "    pred = pred.bool()\n",
        "    target = target.bool()\n",
        "\n",
        "    tp = (pred & target).sum().item()\n",
        "    fp = (pred & ~target).sum().item()\n",
        "    fn = (~pred & target).sum().item()\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-8)\n",
        "    recall    = tp / (tp + fn + 1e-8)\n",
        "    f1        = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "    iou       = tp / (tp + fp + fn + 1e-8)\n",
        "\n",
        "    return precision, recall, f1, iou\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6avn6yiOoGc"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def testing(model, dataloader, device, num_batches=1):\n",
        "    model.eval()\n",
        "\n",
        "    metrics = {\n",
        "        \"cat\": {\"precision\": [], \"recall\": [], \"f1\": [], \"iou\": []},\n",
        "        \"dog\": {\"precision\": [], \"recall\": [], \"f1\": [], \"iou\": []},\n",
        "    }\n",
        "\n",
        "    for batch_idx, (imgs, masks) in enumerate(dataloader):\n",
        "        if batch_idx >= num_batches:\n",
        "            break\n",
        "\n",
        "        imgs = imgs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        outputs = model(imgs)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        imgs = imgs.cpu()\n",
        "        masks = masks.cpu()\n",
        "        preds = preds.cpu()\n",
        "\n",
        "        for i in range(imgs.size(0)):\n",
        "            img = imgs[i].permute(1, 2, 0)\n",
        "            gt = masks[i]\n",
        "            pr = preds[i]\n",
        "\n",
        "            # ===== Metrics =====\n",
        "            # Cat = class 1\n",
        "            p, r, f1, iou = compute_metrics_binary(pr == 1, gt == 1)\n",
        "            metrics[\"cat\"][\"precision\"].append(p)\n",
        "            metrics[\"cat\"][\"recall\"].append(r)\n",
        "            metrics[\"cat\"][\"f1\"].append(f1)\n",
        "            metrics[\"cat\"][\"iou\"].append(iou)\n",
        "\n",
        "            # Dog = class 2\n",
        "            p, r, f1, iou = compute_metrics_binary(pr == 2, gt == 2)\n",
        "            metrics[\"dog\"][\"precision\"].append(p)\n",
        "            metrics[\"dog\"][\"recall\"].append(r)\n",
        "            metrics[\"dog\"][\"f1\"].append(f1)\n",
        "            metrics[\"dog\"][\"iou\"].append(iou)\n",
        "\n",
        "            # ===== Plot =====\n",
        "            fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
        "            fig.suptitle(\n",
        "                f\"Sample {i} | \"\n",
        "                f\"Cat IoU: {metrics['cat']['iou'][-1]:.3f} | \"\n",
        "                f\"Dog IoU: {metrics['dog']['iou'][-1]:.3f}\",\n",
        "                fontsize=14\n",
        "            )\n",
        "\n",
        "            axs[0, 0].imshow(img)\n",
        "            axs[0, 0].set_title(\"Input Image\")\n",
        "            axs[0, 0].axis(\"off\")\n",
        "\n",
        "            axs[0, 1].imshow(gt == 1, cmap=\"gray\")\n",
        "            axs[0, 1].set_title(\"GT - Cat\")\n",
        "            axs[0, 1].axis(\"off\")\n",
        "\n",
        "            axs[0, 2].imshow(gt == 2, cmap=\"gray\")\n",
        "            axs[0, 2].set_title(\"GT - Dog\")\n",
        "            axs[0, 2].axis(\"off\")\n",
        "\n",
        "            axs[1, 1].imshow(pr == 1, cmap=\"gray\")\n",
        "            axs[1, 1].set_title(\"Pred - Cat\")\n",
        "            axs[1, 1].axis(\"off\")\n",
        "\n",
        "            axs[1, 2].imshow(pr == 2, cmap=\"gray\")\n",
        "            axs[1, 2].set_title(\"Pred - Dog\")\n",
        "            axs[1, 2].axis(\"off\")\n",
        "\n",
        "            axs[1, 0].axis(\"off\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    # ===== Aggregate results =====\n",
        "    print(\"\\n===== FINAL METRICS =====\")\n",
        "    for cls in [\"cat\", \"dog\"]:\n",
        "        print(f\"\\nClass: {cls}\")\n",
        "        for m in metrics[cls]:\n",
        "            print(f\"{m}: {sum(metrics[cls][m]) / len(metrics[cls][m]):.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".conda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}