{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cYPZMYrjlay"
      },
      "source": [
        "## **Import libs:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_Z0mreftNrMc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.tv_tensors import Image, Mask\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "import torchvision.transforms.v2 as T\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HSoqh2TQJ2zZ"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "BATCHSIZE = 4\n",
        "LR = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_DVdhVjqTe"
      },
      "source": [
        "## **Check Gpu:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNNT5iJuJxDg",
        "outputId": "87a1d3d1-1292-4ea5-8bbb-6e62ffeb8ddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9YeMxafjxEt"
      },
      "source": [
        "## **Architecture:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pax merging is the equivalent to MaxPool 2x2 is the Unet.\n",
        "I divide the input as an image composed by 4x4 patches.\n",
        "I want to create a image composed by 2x2 patches, so I need to reshape.\n",
        "\n",
        "x.reshape(B, H // 2, 2, W // 2, 2, C) give us (B, H // 2, 2, W // 2, 2, C), but I want B, H//2, W//2, 2C... I need to permute, so I have: B, H//2, W//2, 2, 2, C.\n",
        "After that, I need to reshape to have B, H//2, W//2, 4C, and I need a reducton to have B, H//2, W//2, 2C \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PatchMerging(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim * 4)\n",
        "        self.reduction = nn.Linear(dim * 4, 2*dim, bias = False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, H, W, C = x.shape\n",
        "        x = x.reshape(B, H // 2, 2, W // 2, 2, C).permute(0, 1, 3, 2, 4, 5).reshape(B, H // 2, W // 2, 4 * C)\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need the equivalent of Upsampling: Patch Expanding..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PatchExpanding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.expand = nn.Linear(2 * dim, 4 * dim, bias = False)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x): \n",
        "        # x = B, H /2, W/2, 2C\n",
        "        x = self.expand(x)\n",
        "        # x = B, H/2, W/2, 4C\n",
        "        B, semiH, semiW, Cx4 = x.shape \n",
        "        x = x.view(B, semiH, semiW, 2, 2, Cx4 // 4) \n",
        "        # x = B, H/2, W/2, 2, 2, 4C/4\n",
        "        x = x.permute(0, 1, 3, 2, 4, 5)\n",
        "        # x = B, H/2, 2, W/2, 2, 4C/4\n",
        "        x = x.reshape(B, semiH *2 , semiW * 2,Cx4 // 4)\n",
        "        # x = B, H, W, 4C/4\n",
        "        x = self.norm(x)\n",
        "        return x # B, H, W, C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One of the assets of swin unet, it's this capacity of create tokens from the input, using windows, and doing attention on fixed windows W-MSA vs on swift windows SW-MSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def window_partition(x, window_size):\n",
        "    B, H, W, C = x.shape\n",
        "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
        "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
        "    return windows\n",
        "\n",
        "def window_reverse(windows, window_size, H, W):\n",
        "    C = windows.shape[-1]\n",
        "    x = windows.view(-1, H //window_size, W // window_size, window_size, window_size, C)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, H, W, C)\n",
        "    return x\n",
        "\n",
        "def get_relative_position_index(window_size, device = None):\n",
        "    coords_h = torch.arange(window_size, device= device, dtype=torch.long)\n",
        "    coords_w = torch.arange(window_size, device= device, dtype=torch.long)\n",
        "    coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))  # 2, Wh, Ww\n",
        "    coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "    relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "    relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "    relative_coords[:, :, 0] += window_size - 1  # shift to start from 0\n",
        "    relative_coords[:, :, 1] += window_size - 1\n",
        "    relative_coords[:, :, 0] *= 2 * window_size - 1\n",
        "    relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "    return relative_position_index\n",
        "\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        self.register_buffer(\"relative_position_index\",  get_relative_position_index(window_size))\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads)\n",
        "        )\n",
        "\n",
        "        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B_, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))\n",
        "\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            N, N, -1)\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, N, N)\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "        else:\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Swin Transformer block:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SwinTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = WindowAttention(dim, window_size, num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        self.drop_path = nn.Identity() if drop_path == 0. else nn.Dropout(drop_path)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.Linear(mlp_hidden_dim, dim),\n",
        "            nn.Dropout(drop)\n",
        "        )\n",
        "\n",
        "        if shift_size > 0:\n",
        "            self.register_buffer(\"attention_mask\", self.compute_mask(*input_resolution, device = \"cpu\"))\n",
        "        else:\n",
        "            self.attention_mask = None\n",
        "\n",
        "    def compute_mask(self, H, W, device = None):\n",
        "        img_mask = torch.zeros((1, H, W, 1), device=device)\n",
        "        h_slices = (slice(0, -self.window_size),\n",
        "                    slice(-self.window_size, -self.shift_size),\n",
        "                    slice(-self.shift_size, None))\n",
        "        w_slices = (slice(0, -self.window_size),\n",
        "                    slice(-self.window_size, -self.shift_size),\n",
        "                    slice(-self.shift_size, None))\n",
        "        cnt = 0\n",
        "        for h in h_slices:\n",
        "            for w in w_slices:\n",
        "                img_mask[:, h, w, :] = cnt\n",
        "                cnt += 1\n",
        "\n",
        "        mask_windows = window_partition(img_mask, self.window_size)\n",
        "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
        "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
        "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(\"-inf\")).masked_fill(attn_mask == 0, float(0.0))\n",
        "        return attn_mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.attention_mask is not None:\n",
        "            self.attention_mask = self.attention_mask.to(x.device)\n",
        "        B, H, W, C = x.shape\n",
        "        shortcut = x\n",
        "\n",
        "        x = self.norm1(x)\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        x_windows = window_partition(x, self.window_size)\n",
        "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n",
        "\n",
        "        attn_windows = self.attn(x_windows, mask=self.attention_mask)\n",
        "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
        "\n",
        "        x = window_reverse(attn_windows, self.window_size, H, W)\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "        \n",
        "        x = shortcut + x\n",
        "        x += self.mlp(self.norm2(x))\n",
        "        return x\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Swin Transformer (W-MWSA & SW-MSA):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SwinBlock(nn.Module):\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkIffQ-fPJ5T",
        "outputId": "6abde171-0786-4f77-e6af-61d62be57de1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unet(\n",
            "  (encoder): Encoder(\n",
            "    (blocks): ModuleList(\n",
            "      (0): DoubleConv(\n",
            "        (net): Sequential(\n",
            "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (1): DoubleConv(\n",
            "        (net): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (2): DoubleConv(\n",
            "        (net): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (bottleneck): DoubleConv(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (blocks): ModuleList(\n",
            "      (0): DoubleConv(\n",
            "        (net): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (1): DoubleConv(\n",
            "        (net): Sequential(\n",
            "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (2): DoubleConv(\n",
            "        (net): Sequential(\n",
            "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (up_conv): ModuleList(\n",
            "      (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (1): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
            "    )\n",
            "  )\n",
            "  (head): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias= False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias= False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.net(X)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([DoubleConv(channels[i], channels[i+1]) for i in range (len(channels) - 1)])\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    def forward(self, X):\n",
        "        skips = []\n",
        "        for current_block in (self.blocks):\n",
        "            X = current_block(X)\n",
        "            skips.append(X)\n",
        "            X = self.pool(X)\n",
        "\n",
        "        return X, skips\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([DoubleConv(channels[i + 1] * 2, channels[i+1]) for i in range (len(channels) - 1 )])\n",
        "        self.up_conv = nn.ModuleList([nn.ConvTranspose2d(channels[i], channels[i + 1], 2, 2) for i in range(len(channels) - 1)])\n",
        "\n",
        "    def forward(self, X, skip_connection):\n",
        "        for idx, (current_block, current_up_conv) in enumerate(zip(self.blocks, self.up_conv)):\n",
        "            X = current_up_conv(X)\n",
        "            X = torch.cat([X, skip_connection[-(idx + 1)]], dim=1)\n",
        "            X = current_block(X)\n",
        "\n",
        "        return X\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, nb_class):\n",
        "        super(Unet, self).__init__()\n",
        "        self.encoder = Encoder([3, 64, 128, 256])\n",
        "        self.bottleneck = DoubleConv(256, 512)\n",
        "        self.decoder = Decoder([512, 256, 128, 64])\n",
        "        self.head = nn.Conv2d(64, nb_class, 1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X, skip_connection = self.encoder(X)\n",
        "        X = self.bottleneck(X)\n",
        "        X = self.decoder(X, skip_connection)\n",
        "        X = self.head(X)\n",
        "        return X\n",
        "\n",
        "\n",
        "model= Unet(3).to(device)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wc2sgIXj0Lt"
      },
      "source": [
        "##**DataGenerator:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uram423_j4HU"
      },
      "outputs": [],
      "source": [
        "train_transform = T.Compose([\n",
        "    T.Resize((512, 512)),\n",
        "    T.RandomHorizontalFlip(p = 0.5),\n",
        "    T.RandomRotation(degrees = 15),\n",
        "])\n",
        "\n",
        "val_transform = T.Compose([\n",
        "    T.Resize((512, 512)),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zpdy8rpJkhup"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "import torchvision.transforms.v2 as T\n",
        "\n",
        "\n",
        "\n",
        "class OxfordPetSegmentation(Dataset):\n",
        "    def __init__(self, root, split = \"trainval\", transforms = None):\n",
        "        self.dataset = OxfordIIITPet(root=root, download=True, split = split, target_types = [\"segmentation\", \"category\"])\n",
        "        self.transforms = transforms\n",
        "        self.classes = self.dataset.classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, (mask, label) = self.dataset[idx]\n",
        "\n",
        "        mask_np = np.array(mask)\n",
        "        animal_pixels = (mask_np == 1) \n",
        "\n",
        "        multiclass_mask = np.zeros_like(mask_np, dtype=np.uint8)\n",
        "\n",
        "        breed_name = self.classes[label].lower()\n",
        "        if breed_name in ['abyssinian', 'bengal', 'birman', 'bombay', 'british shorthair',\n",
        "                        'egyptian mau', 'maine coon', 'persian', 'ragdoll', 'russian blue',\n",
        "                        'siamese', 'sphynx']: # Cat\n",
        "            multiclass_mask[animal_pixels] = 1\n",
        "        else:\n",
        "            multiclass_mask[animal_pixels] = 2\n",
        "\n",
        "        image_dp = Image(image)\n",
        "        mask_dp = Mask(multiclass_mask)\n",
        "\n",
        "        # Apply geometric transforms to datapoints\n",
        "        if self.transforms:\n",
        "            # Transforms that work on (dp.Image, dp.Mask) will keep them synchronized\n",
        "            image_dp, mask_dp = self.transforms(image_dp, mask_dp)\n",
        "\n",
        "        image_tensor = T.ToDtype(torch.float32, scale=True)(image_dp)\n",
        "        mask_tensor = torch.as_tensor(mask_dp, dtype=torch.long)\n",
        "\n",
        "        return image_tensor, mask_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWLutCzzDqkp",
        "outputId": "8c1c5080-2c6c-433b-fc7f-52c9f682b468"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 792M/792M [00:29<00:00, 26.7MB/s]\n",
            "100%|██████████| 19.2M/19.2M [00:01<00:00, 12.6MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Abyssinian', 'American Bulldog', 'American Pit Bull Terrier', 'Basset Hound', 'Beagle', 'Bengal', 'Birman', 'Bombay', 'Boxer', 'British Shorthair', 'Chihuahua', 'Egyptian Mau', 'English Cocker Spaniel', 'English Setter', 'German Shorthaired', 'Great Pyrenees', 'Havanese', 'Japanese Chin', 'Keeshond', 'Leonberger', 'Maine Coon', 'Miniature Pinscher', 'Newfoundland', 'Persian', 'Pomeranian', 'Pug', 'Ragdoll', 'Russian Blue', 'Saint Bernard', 'Samoyed', 'Scottish Terrier', 'Shiba Inu', 'Siamese', 'Sphynx', 'Staffordshire Bull Terrier', 'Wheaten Terrier', 'Yorkshire Terrier']\n",
            "['Abyssinian', 'American Bulldog', 'American Pit Bull Terrier', 'Basset Hound', 'Beagle', 'Bengal', 'Birman', 'Bombay', 'Boxer', 'British Shorthair', 'Chihuahua', 'Egyptian Mau', 'English Cocker Spaniel', 'English Setter', 'German Shorthaired', 'Great Pyrenees', 'Havanese', 'Japanese Chin', 'Keeshond', 'Leonberger', 'Maine Coon', 'Miniature Pinscher', 'Newfoundland', 'Persian', 'Pomeranian', 'Pug', 'Ragdoll', 'Russian Blue', 'Saint Bernard', 'Samoyed', 'Scottish Terrier', 'Shiba Inu', 'Siamese', 'Sphynx', 'Staffordshire Bull Terrier', 'Wheaten Terrier', 'Yorkshire Terrier']\n"
          ]
        }
      ],
      "source": [
        "train_datset = OxfordPetSegmentation(root = \"data\", split = \"trainval\", transforms = train_transform)\n",
        "val_datset = OxfordPetSegmentation(root = \"data\", split = \"test\", transforms = val_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA8i7elGMjJg",
        "outputId": "70b5b4db-46a0-4804-8677-d5315bf9786f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 512, 512])\n",
            "torch.Size([512, 512])\n",
            "tensor([0, 2])\n"
          ]
        }
      ],
      "source": [
        "img, mask = train_datset[0]\n",
        "print(img.shape)    # [3, 512, 512]\n",
        "print(mask.shape)   # [512, 512]\n",
        "print(torch.unique(mask))  # tensor([0, 1, 2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcyh-zWDGS_3",
        "outputId": "706733b3-9fc1-4fe4-d856-8947841e4086"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_datset, batch_size = BATCHSIZE, shuffle = True, num_workers= 4)\n",
        "val_loader = DataLoader(val_datset, batch_size = BATCHSIZE, shuffle = True, num_workers= 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTmK29a5I6G-"
      },
      "source": [
        "##**Training:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "G23qjYl5I8bu"
      },
      "outputs": [],
      "source": [
        "# Loss: CrossEntropyLoss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ks5ToMezJJEo"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=\"Train\", leave = False)\n",
        "    for images, masks in pbar:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix({\"loss\": loss.item()})\n",
        "    return running_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SxmjG6yjGbod"
      },
      "outputs": [],
      "source": [
        "def compute_confusion(preds, targets, class_id):\n",
        "    \"\"\"\n",
        "    preds, targets: [B, H, W] torch tensors\n",
        "    class_id: int (1=cat, 2=dog)\n",
        "    \"\"\"\n",
        "    preds_c = preds == class_id\n",
        "    targets_c = targets == class_id\n",
        "\n",
        "    tp = (preds_c & targets_c).sum().item()\n",
        "    fp = (preds_c & ~targets_c).sum().item()\n",
        "    fn = (~preds_c & targets_c).sum().item()\n",
        "\n",
        "    return tp, fp, fn\n",
        "\n",
        "\n",
        "\n",
        "def precision_recall_f1_iou(tp, fp, fn, eps=1e-8):\n",
        "    precision = tp / (tp + fp + eps)\n",
        "    recall    = tp / (tp + fn + eps)\n",
        "    f1        = 2 * precision * recall / (precision + recall + eps)\n",
        "    iou       = tp / (tp + fp + fn + eps)\n",
        "    return precision, recall, f1, iou\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "efYGnA2ZJjHe"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validate_one_epoch(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    stats = {\n",
        "        \"cat\" : {\"tp\": 0, \"fp\": 0, \"fn\": 0},\n",
        "        \"dog\" : {\"tp\": 0, \"fp\": 0, \"fn\": 0}\n",
        "    }\n",
        "\n",
        "\n",
        "    for images, masks in dataloader:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        for class_idx, key in enumerate([\"cat\", \"dog\"]):\n",
        "            tp, fp, fn = compute_confusion(preds, masks, class_idx + 1) # Because class are 1 and 2\n",
        "            stats[key][\"tp\"] += tp\n",
        "            stats[key][\"fp\"] += fp\n",
        "            stats[key][\"fn\"] += fn\n",
        "    metrics = {}\n",
        "    for key in [\"cat\", \"dog\"]:\n",
        "        tp = stats[key][\"tp\"]\n",
        "        fp = stats[key][\"fp\"]\n",
        "        fn = stats[key][\"fn\"]\n",
        "\n",
        "        precision, recall, f1, iou = precision_recall_f1_iou(tp, fp, fn, eps = 1e-8)\n",
        "        metrics[key] = {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"iou\": iou,\n",
        "        }\n",
        "    return running_loss / len(dataloader), metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "qdOzI_XYJxu4",
        "outputId": "69115e1c-b205-45b2-9ef4-2a07c448b737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1954637135.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     mean_iou = (\n",
            "\u001b[0;32m/tmp/ipython-input-715050043.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "best_iou = 0.0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_metrics = validate_one_epoch(model, val_loader, criterion, device)\n",
        "    mean_iou = (\n",
        "        val_metrics[\"cat\"][\"iou\"] +\n",
        "        val_metrics[\"dog\"][\"iou\"]\n",
        "    ) / 2\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"Val   Loss: {val_loss:.4f}\")\n",
        "    print(\n",
        "        f\"Mean IoU: {mean_iou:.3f} | \"\n",
        "        f\"Cat IoU: {val_metrics['cat']['iou']:.3f} | \"\n",
        "        f\"Dog IoU: {val_metrics['dog']['iou']:.3f}\"\n",
        "    )\n",
        "\n",
        "    for cls in [\"cat\", \"dog\"]:\n",
        "        m = val_metrics[cls]\n",
        "        print(\n",
        "            f\"{cls.upper()} → \"\n",
        "            f\"P: {m['precision']:.3f} \"\n",
        "            f\"R: {m['recall']:.3f} \"\n",
        "            f\"F1: {m['f1']:.3f}\"\n",
        "        )\n",
        "\n",
        "    if mean_iou > best_iou:\n",
        "        best_iou = mean_iou\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(\"Best model saved\")\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), \"final_model.pth\")\n",
        "\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XwooMw2KHEh"
      },
      "source": [
        "##**Testing:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roJTeDvNKJ6E"
      },
      "outputs": [],
      "source": [
        "def compute_metrics_binary(pred, target):\n",
        "    pred = pred.bool()\n",
        "    target = target.bool()\n",
        "\n",
        "    tp = (pred & target).sum().item()\n",
        "    fp = (pred & ~target).sum().item()\n",
        "    fn = (~pred & target).sum().item()\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-8)\n",
        "    recall    = tp / (tp + fn + 1e-8)\n",
        "    f1        = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "    iou       = tp / (tp + fp + fn + 1e-8)\n",
        "\n",
        "    return precision, recall, f1, iou\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6avn6yiOoGc"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def testing(model, dataloader, device, num_batches=1):\n",
        "    model.eval()\n",
        "\n",
        "    metrics = {\n",
        "        \"cat\": {\"precision\": [], \"recall\": [], \"f1\": [], \"iou\": []},\n",
        "        \"dog\": {\"precision\": [], \"recall\": [], \"f1\": [], \"iou\": []},\n",
        "    }\n",
        "\n",
        "    for batch_idx, (imgs, masks) in enumerate(dataloader):\n",
        "        if batch_idx >= num_batches:\n",
        "            break\n",
        "\n",
        "        imgs = imgs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        outputs = model(imgs)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        imgs = imgs.cpu()\n",
        "        masks = masks.cpu()\n",
        "        preds = preds.cpu()\n",
        "\n",
        "        for i in range(imgs.size(0)):\n",
        "            img = imgs[i].permute(1, 2, 0)\n",
        "            gt = masks[i]\n",
        "            pr = preds[i]\n",
        "\n",
        "            # ===== Metrics =====\n",
        "            # Cat = class 1\n",
        "            p, r, f1, iou = compute_metrics_binary(pr == 1, gt == 1)\n",
        "            metrics[\"cat\"][\"precision\"].append(p)\n",
        "            metrics[\"cat\"][\"recall\"].append(r)\n",
        "            metrics[\"cat\"][\"f1\"].append(f1)\n",
        "            metrics[\"cat\"][\"iou\"].append(iou)\n",
        "\n",
        "            # Dog = class 2\n",
        "            p, r, f1, iou = compute_metrics_binary(pr == 2, gt == 2)\n",
        "            metrics[\"dog\"][\"precision\"].append(p)\n",
        "            metrics[\"dog\"][\"recall\"].append(r)\n",
        "            metrics[\"dog\"][\"f1\"].append(f1)\n",
        "            metrics[\"dog\"][\"iou\"].append(iou)\n",
        "\n",
        "            # ===== Plot =====\n",
        "            fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
        "            fig.suptitle(\n",
        "                f\"Sample {i} | \"\n",
        "                f\"Cat IoU: {metrics['cat']['iou'][-1]:.3f} | \"\n",
        "                f\"Dog IoU: {metrics['dog']['iou'][-1]:.3f}\",\n",
        "                fontsize=14\n",
        "            )\n",
        "\n",
        "            axs[0, 0].imshow(img)\n",
        "            axs[0, 0].set_title(\"Input Image\")\n",
        "            axs[0, 0].axis(\"off\")\n",
        "\n",
        "            axs[0, 1].imshow(gt == 1, cmap=\"gray\")\n",
        "            axs[0, 1].set_title(\"GT - Cat\")\n",
        "            axs[0, 1].axis(\"off\")\n",
        "\n",
        "            axs[0, 2].imshow(gt == 2, cmap=\"gray\")\n",
        "            axs[0, 2].set_title(\"GT - Dog\")\n",
        "            axs[0, 2].axis(\"off\")\n",
        "\n",
        "            axs[1, 1].imshow(pr == 1, cmap=\"gray\")\n",
        "            axs[1, 1].set_title(\"Pred - Cat\")\n",
        "            axs[1, 1].axis(\"off\")\n",
        "\n",
        "            axs[1, 2].imshow(pr == 2, cmap=\"gray\")\n",
        "            axs[1, 2].set_title(\"Pred - Dog\")\n",
        "            axs[1, 2].axis(\"off\")\n",
        "\n",
        "            axs[1, 0].axis(\"off\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    # ===== Aggregate results =====\n",
        "    print(\"\\n===== FINAL METRICS =====\")\n",
        "    for cls in [\"cat\", \"dog\"]:\n",
        "        print(f\"\\nClass: {cls}\")\n",
        "        for m in metrics[cls]:\n",
        "            print(f\"{m}: {sum(metrics[cls][m]) / len(metrics[cls][m]):.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
